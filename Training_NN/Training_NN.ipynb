{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unstable Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we start this season with the probelm of gradients and how to prevent them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Xavier / Glorot method :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we have 2 conditions to satisfy :\n",
    "\n",
    "1) for each layer, the variance of the inpout and output should remain the same\n",
    "\n",
    "2) the gradient, before and after a layer should maintain its variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 2 cannot happen togheter (which would cause fan_in and fan_out to be equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we have to initialize the weights by :\n",
    "\n",
    "A) Natural distribution, with a certain varience and mean of 0\n",
    "\n",
    "B) Uniform distribution, between r and -r (r = rad(3 * (varience ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if fan_in = fan_out, we would have the LeCun initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has the first method as the default, but:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.dense.Dense at 0x232566f4210>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for furthur personalization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.dense.Dense at 0x232564a9f50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he_avg_init = keras.initializers.VarianceScaling(scale=2, mode=\"fan_avg\", distribution=\"uniform\")\n",
    "\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another place we can look for the cause of this problem would be the activation function\n",
    "\n",
    "because of the fact that mother nature has used the sigmoid activation function, we believed that it was best\n",
    "\n",
    "but as the ReLU activation function will not be saturated (by definition mind you), it did better than sigmoid in most cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Saturated Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as a sneak peak :\n",
    "\n",
    "SELU > ELU > PRelU > ReLU > tanh > logestic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets implement a simple leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential(\n",
    "    # some layers\n",
    "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.LeakyReLU(alpha=0.2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, using the lecun initializer is neccesary for the SELU model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization in Neural Networks\n",
    "\n",
    "Batch Normalization (BN) is a technique used in neural networks to normalize the inputs of each layer. It operates on a mini-batch of data during training and normalizes the activations to have zero mean and unit variance. This is done by subtracting the batch mean and dividing by the batch standard deviation.\n",
    "\n",
    "## Steps of Batch Normalization\n",
    "\n",
    "1. **Normalization:** It standardizes the inputs of a layer, ensuring that they have roughly zero mean and unit variance. This helps in overcoming the internal covariate shift, making training more stable.\n",
    "\n",
    "2. **Scaling and Shifting:** After normalization, the values are scaled and shifted using learnable parameters (gamma and beta). This allows the model to adapt and learn the optimal scale and shift for each feature.\n",
    "\n",
    "## Preventing the Unstable Gradient Problem\n",
    "\n",
    "Batch Normalization helps prevent the unstable gradient problem through:\n",
    "\n",
    "1. **Mitigating Internal Covariate Shift:** By normalizing the inputs, BN reduces the internal covariate shift, which is the change in the distribution of network activations due to parameter updates during training. This helps in stabilizing the training process.\n",
    "\n",
    "2. **Stabilizing Gradients:** BN reduces the dependency of the gradient on the scale of the parameters. During backpropagation, gradients are less likely to vanish or explode as the inputs are within a certain range (near zero mean and unit variance). This mitigates the unstable gradient problem and allows for more effective learning.\n",
    "\n",
    "3. **Enabling Higher Learning Rates:** BN often allows for the use of higher learning rates during training. With normalized inputs, the optimization process is less sensitive to the choice of learning rate, leading to faster convergence.\n",
    "\n",
    "In summary, Batch Normalization is effective in preventing the unstable gradient problem by normalizing inputs, reducing internal covariate shift, and stabilizing the gradients during backpropagation. This, in turn, facilitates more stable and efficient training of neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough talk, lets code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind we have to inmplement the funnel method with the neural numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that we have Batch Normalization, in each and every layer but the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 784)               3136      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 300)               1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271346 (1.04 MB)\n",
      "Trainable params: 268978 (1.03 MB)\n",
      "Non-trainable params: 2368 (9.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in a network this thin, we dont expect much improvment though"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-trainable params are the mean and varience of the batch we have at that instance which are not trainable BY BACKPROPAGATION, which makes them non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wether we decide to use the batch normalization layer before or after the actual layer, is completly based on the problem itself and is determined based on trial and error \n",
    "\n",
    "and note that if we were to use it before, unlike what we did before, i would be :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have to seperate the activation, note that the input layer did not have any activation so we did not have to change that part in any way :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the placement of the BN layer is with respect to an activation layer (wether it be in the by itself, or in a dense layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Activation(\"elu\"), \n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Activation(\"elu\"), \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the hyper parameters of the BN layer we can alter are not much and is best to leave them as are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "worse case, we alter the momentum hyperparameter, which updates the moving average "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the bigger the data and smaller each batch, this parameter should be intensly closer to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Clipping in Neural Networks\n",
    "\n",
    "Gradient clipping is a technique used to address the exploding gradient problem during the training of neural networks. This problem occurs when gradients become extremely large, leading to unstable training and potential divergence. Gradient clipping helps control the magnitude of gradients to prevent this issue.\n",
    "\n",
    "## Gradient Clipping Process\n",
    "\n",
    "1. **Compute Gradients:** During backpropagation, gradients are calculated for each parameter in the neural network.\n",
    "\n",
    "2. **Calculate Gradient Norm:** Calculate the Euclidean norm (L2 norm) of the entire gradient vector. This norm represents the overall magnitude of the gradients.\n",
    "\n",
    "3. **Clip Gradients:** If the calculated norm exceeds a predefined threshold (clip_value), then scale down the entire gradient vector to ensure its norm is within an acceptable range.\n",
    "\n",
    "4. **Update Parameters:** Finally, use the clipped gradients to update the model parameters.\n",
    "\n",
    "## Preventing Exploding Gradients\n",
    "\n",
    "Gradient clipping helps prevent the exploding gradient problem, especially in recurrent neural networks (RNNs) and deep networks. By controlling the magnitude of gradients, it stabilizes the training process and allows the model to learn more effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what this does, is to cut off the gradient vector between -1 and 1\n",
    "\n",
    "but this could be like 0.9 and 1, which would result in the vector to be more in direction of the second axis (which could be problematic)\n",
    "\n",
    "the method clipnorm though, uses the concept of second norm (hing) which is symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning in Neural Networks\n",
    "\n",
    "Transfer learning is a powerful technique in neural network training that leverages pre-trained models to improve the performance of a model on a new, related task. Instead of training a neural network from scratch, transfer learning involves using a pre-trained model's knowledge and adapting it to a different but related problem.\n",
    "\n",
    "## Steps of Transfer Learning\n",
    "\n",
    "1. **Select a Pre-trained Model:** Choose a pre-trained model that has been trained on a large dataset for a similar task. Popular models include VGG, ResNet, Inception, and BERT, depending on the type of task (image classification, object detection, natural language processing, etc.).\n",
    "\n",
    "2. **Remove Last Layers (Optional):** Depending on the similarity of the new task, you might need to remove the last layers of the pre-trained model. For example, in image classification, you may remove the output layer and add a new one with the appropriate number of classes for your task.\n",
    "\n",
    "3. **Freeze Pre-trained Layers (Optional):** Optionally, freeze the weights of the pre-trained layers to prevent them from being updated during the initial training on the new task. This is useful when the lower layers capture generic features that are likely to be beneficial for the new task.\n",
    "\n",
    "4. **Add New Layers:** Add new layers or modify the existing ones to adapt the pre-trained model to the specifics of your task. These new layers are typically randomly initialized and then trained on the new dataset.\n",
    "\n",
    "5. **Training on the New Task:** Train the modified model on your target dataset. Since the pre-trained layers already contain valuable features, training is often faster, and the model can achieve good performance with less data.\n",
    "\n",
    "## Benefits of Transfer Learning\n",
    "\n",
    "- **Faster Training:** Utilizing pre-trained weights speeds up the training process, especially when dealing with large and complex models.\n",
    "\n",
    "- **Improved Generalization:** Transfer learning allows models to generalize well to new tasks, even with limited data for the specific task.\n",
    "\n",
    "- **Effective Feature Extraction:** Pre-trained models serve as effective feature extractors, capturing useful hierarchical features that can be adapted to different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when using transfer learning, we should check for the input shapes to be the same, as this could distrub the training procces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also, the higher we go in the original network, the more making changes could be beneficial\n",
    "\n",
    "with the changing of the output layer being almost mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we start by freezing the layers of the base network, and then starting by the first one or two at the top\n",
    "\n",
    "__while potentialy lowering the learning rate__\n",
    "\n",
    "we start defrosting the layers, training our model\n",
    "\n",
    "if this did not work lower the nunber of layers and repeat this proccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following problem\n",
    "\n",
    "\n",
    "with have a model for the mnist fashion problem (with 8 classes though)\n",
    "and what we want is a binary classification for the two remaining classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the initial model for the mnist fashion problem :\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep in mind that from here on the changes made to model_B_on_A are going to affect model_A\n",
    "\n",
    "\n",
    "to prevent this use clone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets start freezing and training the rest of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep in mind we should compile the model again each time we freez / unfreez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model_B_on_A.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=\"sgd\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the rest of the model now :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_B = y_train_B = X_val_B = y_val_B = []\n",
    "\n",
    "history = model_B_on_A.fit(\n",
    "    X_train_B, \n",
    "    y_train_B, \n",
    "    epochs=4, \n",
    "    validation_data=(X_val_B, y_val_B)\n",
    ")\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=1e-4)\n",
    "model_B_on_A.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=\"sgd\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model_B_on_A.fit(\n",
    "    X_train_B, \n",
    "    y_train_B, \n",
    "    epochs=16, \n",
    "    validation_data=(X_val_B, y_val_B)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well but keep in mind :\n",
    "\n",
    "this method does not work well dense small networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Pretraining in Neural Networks\n",
    "\n",
    "Unsupervised pretraining is a technique in neural network training where a model is pretrained on a task that doesn't require labeled data. This pretrained model can then be fine-tuned on a specific task that does have labeled data, providing a good initialization for the network's weights.\n",
    "\n",
    "## Steps of Unsupervised Pretraining\n",
    "\n",
    "1. **Select an Unsupervised Task:** Choose an unsupervised task that doesn't require labeled data. Examples include autoencoders, denoising autoencoders, or generative models like variational autoencoders (VAEs) or Generative Adversarial Networks (GANs).\n",
    "\n",
    "2. **Pretrain the Model:** Train the neural network on the selected unsupervised task using a large dataset. The goal is for the model to learn useful representations or features from the data without relying on labeled information.\n",
    "\n",
    "3. **Save Pretrained Model Weights:** After unsupervised pretraining, save the weights of the pretrained model. These weights will serve as the starting point for the subsequent fine-tuning on a supervised task.\n",
    "\n",
    "4. **Fine-Tune on a Supervised Task:** Initialize a new neural network with the pretrained weights and fine-tune it on a task that requires labeled data. This can include tasks like image classification, object detection, or sentiment analysis.\n",
    "\n",
    "5. **Training on the Supervised Task:** Train the fine-tuned model on the labeled dataset. The pretrained features help the model converge faster and often lead to better performance compared to training from scratch.\n",
    "\n",
    "## Benefits of Unsupervised Pretraining\n",
    "\n",
    "- **Feature Learning:** Unsupervised pretraining allows the model to learn meaningful features or representations from the data, even when labeled information is not available.\n",
    "\n",
    "- **Improved Generalization:** The learned features can be transferable to various downstream tasks, enhancing the model's ability to generalize.\n",
    "\n",
    "- **Addressing Data Scarcity:** Unsupervised pretraining is particularly useful when labeled data is limited or expensive to obtain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum Optimizer in Neural Networks\n",
    "\n",
    "The Momentum optimizer is an extension of the standard Gradient Descent (GD) optimization algorithm that helps accelerate training and navigate through areas with noisy or sparse gradients.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Standard Gradient Descent (GD)\n",
    "\n",
    "In standard GD, the model parameters are updated in the opposite direction of the gradient of the loss with respect to those parameters. The update rule for each parameter (θ) at each iteration is given by:\n",
    "\n",
    "\\[ \\theta_{t+1} = \\theta_t - \\alpha \\nabla L(\\theta_t) \\]\n",
    "\n",
    "where:\n",
    "- \\( \\alpha \\) is the learning rate.\n",
    "- \\( \\nabla L(\\theta_t) \\) is the gradient of the loss function with respect to the parameters.\n",
    "\n",
    "### Momentum Optimizer\n",
    "\n",
    "The Momentum optimizer introduces the concept of a \"momentum\" term to the parameter updates. The update rule for each parameter (θ) at each iteration is given by:\n",
    "\n",
    "\\[ v_{t+1} = \\beta v_t + (1 - \\beta) \\nabla L(\\theta_t) \\]\n",
    "\\[ \\theta_{t+1} = \\theta_t - \\alpha v_{t+1} \\]\n",
    "\n",
    "where:\n",
    "- \\( \\alpha \\) is the learning rate.\n",
    "- \\( \\nabla L(\\theta_t) \\) is the gradient of the loss function with respect to the parameters.\n",
    "- \\( v_t \\) is the momentum term at time step \\( t \\).\n",
    "- \\( \\beta \\) is the momentum coefficient, typically close to 1 (e.g., 0.9).\n",
    "\n",
    "## Differences Between Momentum and GD\n",
    "\n",
    "1. **Acceleration:** Momentum helps accelerate training, especially in the presence of oscillations or noisy gradients. The momentum term allows the optimizer to accumulate velocity in directions with consistent gradients, enabling faster convergence.\n",
    "\n",
    "2. **Inertia:** The momentum term introduces an \"inertia\" effect, allowing the optimizer to continue moving in the previous direction, even if the gradient changes direction or magnitude. This helps the optimizer navigate through flat regions or saddle points more efficiently.\n",
    "\n",
    "3. **Damping Effect:** The momentum term acts as a damping factor for oscillations. It reduces the impact of oscillations or high-frequency noise in the gradient, leading to smoother and more stable updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beta hyperparameter, is a moving average of gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What determines a step :\n",
    "\n",
    "1) GD : how far from the answer we are\n",
    "\n",
    "2) MO : how flat the plane is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the implementation is easy :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MO tends to act like damped oscillation having fluctuation around the answer, and it is much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Schedules in Neural Networks\n",
    "\n",
    "## Learning Rate Schedule Overview\n",
    "\n",
    "In the context of neural network training, a learning rate schedule refers to the dynamic adjustment of the learning rate during training. The learning rate is a hyperparameter that determines the size of the steps taken during optimization. Different schedules can impact the convergence speed, stability, and generalization of the model.\n",
    "\n",
    "## 1. Power Scheduling\n",
    "\n",
    "Power scheduling adjusts the learning rate based on a power of the iteration number. The learning rate at each iteration (\\(t\\)) is calculated as:\n",
    "\n",
    "\\[ \\alpha_t = \\alpha_0 \\cdot \\frac{1}{(1 + t \\cdot k)^p} \\]\n",
    "\n",
    "This schedule is effective for gradually reducing the learning rate, allowing the model to converge more slowly over time.\n",
    "\n",
    "## 2. Exponential Scheduling\n",
    "\n",
    "Exponential scheduling reduces the learning rate exponentially over iterations. The learning rate at each iteration (\\(t\\)) is calculated as:\n",
    "\n",
    "\\[ \\alpha_t = \\alpha_0 \\cdot e^{-kt} \\]\n",
    "\n",
    "This schedule rapidly reduces the learning rate, promoting faster convergence initially but may become very small in later iterations.\n",
    "\n",
    "## 3. Fixed Step Scheduling\n",
    "\n",
    "Fixed step scheduling keeps the learning rate constant throughout training. The learning rate at each iteration (\\(t\\)) is constant:\n",
    "\n",
    "\\[ \\alpha_t = \\alpha_0 \\]\n",
    "\n",
    "While simple, this schedule may not be optimal for all scenarios and often requires careful tuning of the initial learning rate.\n",
    "\n",
    "## 4. Performance Scheduling\n",
    "\n",
    "Performance scheduling adjusts the learning rate based on the model's performance. If the validation error stops improving, the learning rate is reduced. The learning rate at each iteration (\\(t\\)) is calculated as:\n",
    "\n",
    "\\[ \\alpha_t = \\alpha_0 \\cdot \\text{factor}^{\\text{epoch\\_no\\_improvement}} \\]\n",
    "\n",
    "This schedule adapts the learning rate based on the model's performance on the validation set.\n",
    "\n",
    "## 5. One-Cycle Scheduling\n",
    "\n",
    "One-Cycle scheduling involves a cyclical learning rate, where the learning rate starts low, gradually increases to a maximum, and then decreases again. This is done within a single cycle of training. The learning rate at each iteration (\\(t\\)) is calculated using a piecewise linear or cosine annealing function.\n",
    "\n",
    "This schedule is designed to achieve both fast convergence and fine-tuning by exploring a broad range of learning rates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now lets look at some implementations :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power Scheduling :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential Scheduling :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1 ** (epoch / 20)\n",
    "\n",
    "# or more percisly : \n",
    "\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return 0.01 * 0.1 ** (epoch / 20)\n",
    "    return exponential_decay_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = y_train = []\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train, \n",
    "    # [...]\n",
    "    callbacks=[lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or quit simply :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1 ** (epoch / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When saving a model, the optimizer and the learning rate are also saved with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we can continue an unfinished training with this different optimization's "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is not the case when we have the parameter \"epoch\", as it is not saved\n",
    "\n",
    "so dont interupt the training proccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed Step Scheduling :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecwise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecwise_constant_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Scheduling :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so on...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over fitting prevention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Normalization (Lasso Regularization)\n",
    "\n",
    "L1 normalization, also known as Lasso regularization, is a technique used to regularize neural network models. It adds a penalty term to the loss function proportional to the absolute values of the model weights.\n",
    "\n",
    "\n",
    "## L2 Normalization (Ridge Regularization)\n",
    "\n",
    "L2 normalization, also known as Ridge regularization, is another technique used to regularize neural network models. It adds a penalty term to the loss function proportional to the squared values of the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(\n",
    "    100, \n",
    "    activation='elu', \n",
    "    kernel_initializer=\"he_normal\", \n",
    "    kernel_regularizer=keras.regularizers.l2(0.01)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The l2 regularizer basicly makes the weights more sparse, punishing when a weight is to large or small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also can use a combination of both with l1_l2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and as the basic features of each layer are simply repeated we can :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "RegularizedDense = partial(\n",
    "    keras.layers.Dense,\n",
    "    activation=\"elu\", \n",
    "    kernel_initializer=\"he_normal\", \n",
    "    kernel_regularizer=keras.regularizers.l2(0.01)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Regularization\n",
    "\n",
    "Dropout is a regularization technique used in neural networks to prevent overfitting. It involves randomly setting a fraction of input units to zero during training, which helps prevent complex co-adaptations on training data.\n",
    "\n",
    "## How Dropout Works\n",
    "\n",
    "At each training step, Dropout randomly \"drops out\" (sets to zero) a fraction of the input units, chosen at random. This helps to prevent overfitting by ensuring that no single neuron becomes too specialized, and the network becomes more robust.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "The mathematical formulation of Dropout involves applying a binary mask to the output of the layer during training. The mask is generated independently for each input unit at each update of the training phase. The output \\(y\\) is given by:\n",
    "\n",
    "\\[ y = \\frac{x \\cdot \\text{mask}}{1 - \\text{dropout\\_rate}} \\]\n",
    "\n",
    "where:\n",
    "- \\(x\\) is the input to the layer.\n",
    "- \\(\\text{mask}\\) is the binary mask.\n",
    "- \\(\\text{dropout\\_rate}\\) is the fraction of units to drop.\n",
    "\n",
    "During testing or inference, no units are dropped, but the output is scaled by a factor of \\(1 - \\text{dropout\\_rate}\\) to maintain the expected output magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method has the advantages of almost always making the model better\n",
    "\n",
    "and by definition it cannot make it worse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    keras.layers.Dropout(rate=0.2), \n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.Dropout(rate=0.2), \n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.Dropout(rate=0.2), \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is overfitting, increase the rate, and wise-versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Dropout\n",
    "\n",
    "Monte Carlo Dropout is an extension of the Dropout regularization technique that involves making multiple predictions with dropout-enabled models at test time. It provides a form of uncertainty estimation and can be useful for tasks where understanding model uncertainty is crucial.\n",
    "\n",
    "## How Monte Carlo Dropout Works\n",
    "\n",
    "In traditional Dropout, during training, a fraction of input units is randomly set to zero. In Monte Carlo Dropout, this dropout behavior is retained during testing or inference. Instead of making a single deterministic prediction, the model is sampled multiple times with dropout applied, and predictions are averaged.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "Mathematically, let \\(f(x)\\) represent the prediction of the model for input \\(x\\). In Monte Carlo Dropout, the prediction is obtained by averaging over multiple dropout samples:\n",
    "\n",
    "\\[ \\text{Prediction} = \\frac{1}{N} \\sum_{i=1}^{N} f(x) \\]\n",
    "\n",
    "where:\n",
    "- \\(N\\) is the number of dropout samples.\n",
    "\n",
    "This process provides an estimate of the model's uncertainty, and the variance in predictions reflects the uncertainty in the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_test_scaled = []\n",
    "\n",
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple as that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "monte carlo does not alway give us better results \n",
    "\n",
    "but better ones "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max-Norm Regularization\n",
    "\n",
    "Max-Norm regularization is a technique used to prevent the weights in a neural network from becoming too large during training. This regularization method imposes a constraint on the maximum L2 norm of the weight vectors in a layer.\n",
    "\n",
    "## How Max-Norm Regularization Works\n",
    "\n",
    "1. **Weight Normalization:**\n",
    "   - For each neuron in a layer, the weights are normalized. The L2 norm (Euclidean norm) of the weight vector is calculated.\n",
    "\n",
    "2. **Applying Constraint:**\n",
    "   - The L2 norm is then compared to a predefined threshold, denoted as \\(c\\) (the max-norm constraint).\n",
    "   - If the L2 norm exceeds \\(c\\), the weight vector is scaled down to ensure that the constraint is satisfied:\n",
    "\n",
    "     \\[ \\|W_i\\|_2 \\leq c \\]\n",
    "\n",
    "   where:\n",
    "   - \\(\\|W_i\\|_2\\) is the L2 norm of the weight vector for neuron \\(i\\).\n",
    "   - \\(c\\) is the max-norm constraint.\n",
    "\n",
    "## Why Max-Norm Regularization?\n",
    "\n",
    "### 1. Improved Generalization:\n",
    "   - Large weights in a neural network can lead to overfitting, where the model becomes too specialized to the training data and performs poorly on new, unseen data.\n",
    "   - Max-Norm regularization prevents the weights from growing excessively, promoting better generalization to new data.\n",
    "\n",
    "### 2. Stability During Training:\n",
    "   - Prevents exploding gradients: Large weights can cause the gradients during backpropagation to become very large, leading to instability in training. Max-Norm regularization helps mitigate this issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets see a breif implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.dense.Dense at 0x2325b4bbcd0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(\n",
    "    100, \n",
    "    activation=\"elu\", \n",
    "    kernel_initializer=\"he_normal\", \n",
    "    kernel_constraint=keras.constraints.max_norm(1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
